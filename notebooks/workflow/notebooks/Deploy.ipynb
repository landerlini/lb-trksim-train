{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af061e96-0b00-405a-832a-018f54ccc859",
   "metadata": {},
   "source": [
    "# Deployment as a C-compiled shared library\n",
    "\n",
    "In order to run the trained parametrizations within a Gaudi application,\n",
    "some transformation to a format that can be easily loaded from C/C++.\n",
    "Several options were considered, but most of them require a dedicated runtime\n",
    "to be deployed and linked in the target application.\n",
    "\n",
    "In practice, the runtimes are extremely effective to distribute the\n",
    "computation on multiple cores and are designed to speed up the evaluation\n",
    "of the networks on large batches of data samples.\n",
    "Unfortunately, in Lamarr the number of particles we can process per\n",
    "event is rather limited and the complexity of the models is also not\n",
    "comparable to those used for Computer Vision or Natural Language Processing\n",
    "tasks. As a consequence, the overhead introduced for context switching\n",
    "when passing the input variables to the main thread to the\n",
    "runtime thread is unacceptable large.\n",
    "In addition, the latest versions of Gaudi introduce multithread\n",
    "processing that ease to reduce the memory footprint of the applications.\n",
    "Unfortunately, the schedulers designed for runtime use a completly different\n",
    "logic from the Gaudi scheduler and are cerainly not designed to interact\n",
    "effectively with Gaudi. In conclusion, the attempts of relying on the\n",
    "TensorFlow C++ APIs and a first exploration of using ONNX for Lamarr,\n",
    "convinced the Simulation Project to move towards other alternatives.\n",
    "\n",
    "A totally different approach, developed in the context of real time\n",
    "processing for plasma control in nuclear fusion experiments is to translate\n",
    "the neural network in compatible C code, compile it, and link it\n",
    "to the main application.\n",
    "The most important effort in this direction lead to the release of\n",
    "[keras2c](https://github.com/f0uriest/keras2c).\n",
    "\n",
    "In LHCb, we have developed our own version of a transpiler of *scikit-learn*\n",
    "and Keras models to compatible C code, under the name\n",
    "[`scikinC`](https://github.com/landerlini/scikinC).\n",
    "While much more limited in terms of support for neural networks, scikinC\n",
    "provides functionalities to combine the preprocessing step with the model\n",
    "in a very simple and clean way.\n",
    "\n",
    "In this notebook we will run a development version of scikinC as some\n",
    "of the functionalities needed to deploy these neural networks are being\n",
    "implemented now.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c83f34c-2967-41fb-bd7b-61f79f563858",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Importng scikinC\n",
    "import scikinC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060cf47b-95f8-4beb-85ef-05d8a5a6b152",
   "metadata": {},
   "source": [
    "## Libraries and environment\n",
    "The libraries used to import the model and the validate model and deployment\n",
    "are the same as for the validation notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bd02116e-65ee-420a-8dc8-27d4373eb12c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753088668.587705  676042 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753088668.637616  676042 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1753088669.105551  676042 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753088669.105638  676042 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753088669.105640  676042 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1753088669.105642  676042 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os, os.path\n",
    "from os import environ\n",
    "\n",
    "## Remove annoying warnings \n",
    "environ['TF_CPP_MIN_LOG_LEVEL'] = '3' \n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c677add-8bfc-4196-88a4-19c8726f9da7",
   "metadata": {},
   "source": [
    "## Loading and collapsing the models\n",
    "In this notebook we are going to load four different models for:\n",
    " * acceptance,\n",
    " * efficiency,\n",
    " * resolution,\n",
    " * covariance.\n",
    "\n",
    "For each of these models, we are loading:\n",
    " * a preprocessing step (named `tX`)\n",
    " * a Deep Neural Network model\n",
    "\n",
    "for the GAN models (resolution and covariance) we are loading a\n",
    "*postprocessing* step (named `tY`) as well.\n",
    "To handle the model loading and some manipulation needed to make it\n",
    "ready to be deployed, we rely on a custom class, named `LamarrModel`\n",
    "defined in the local module [`deploy_utils.py`](./deploy_utils.py).\n",
    "The class `LamarrModel` takes as an input the path of a keras model,\n",
    "and loads from the corresponding folder:\n",
    " * the Neural Network model\n",
    " * if a `tX.pkl` file is found, it loads it as a pickled preprocessing step\n",
    " * if a `tY.pkl` file is found, it loads it as a pickled postprocessing step\n",
    "\n",
    "Then, it tries to convert the model to a keras Sequential model.\n",
    "Note that most of the neural networks defined in this repository include\n",
    "skip connections and are therefore defined with the Functional APIs.\n",
    "As of today, models defined with the Functional APIs cannot be converted\n",
    "with scikinC, hence, a conversion of each DNN to a Sequential model is\n",
    "necessary.\n",
    "Note that the restriction to the Sequential form is formal, as long as\n",
    "we do not constrain the operations single layers may perform.\n",
    "\n",
    "In the particular case of dense layers with skip connections, we can\n",
    "define a new layer, named `DenseWithSkipConnection` that implements the\n",
    "operation\n",
    "$$\n",
    "\\vec h_{i+1} = \\vec h_{i} + \\sigma(\\mathbf{A} \\vec h_{i} + \\vec b),\n",
    "$$\n",
    "where $\\mathbf{A}$ and $\\vec {b}$ represent the weights of the dense\n",
    "layer and $\\sigma(\\cdot)$ the activation function.\n",
    "A stack of `DenseWithSkipConnection` layers can reproduce exactly the\n",
    "functionality of the original neural network while being represented as\n",
    "a Sequential model.\n",
    "\n",
    "To be completely honest, this simply moves the problem to `scikinC`\n",
    "that has now to provide a C implementation for our custom\n",
    "`DenseWithSkipConnection` layer.\n",
    "Fortunately, this is extremely immediate to implement (though a bit *hacky*),\n",
    "requiring only few lines of Python, placed in the\n",
    "[`deploy_utils.py`](./deploy_utils.py) module.\n",
    "\n",
    "So, in summary, we are defining:\n",
    " * a custom layer named `DenseWithSkipConnection` that will replace any combination\n",
    "   of a keras Dense layer followed by a skip connection;\n",
    " * a custom scikinC implementation for `DenseWithSkipConnection` that implements\n",
    "   this layer in C, reusing the C code for converting Dense layers as much as possible.\n",
    "\n",
    "Most of this happens behind the scenes in the implementation of the `LamarrModel`\n",
    "class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "64666e46-6e4e-417a-895b-5357c28dcdcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from deploy_utils import LamarrModel, hacks\n",
    "\n",
    "## Definition of a custom layer converter in scikinC\n",
    "import scikinC.layers\n",
    "scikinC.layers.DenseWithSkipConnection = hacks.scikinC_DenseWithSkipConnection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9aa25d2c-9849-4692-91f8-fac8b5646660",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator QuantileTransformer from version 1.7.0 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator FunctionTransformer from version 1.7.0 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/sklearn/base.py:442: InconsistentVersionWarning: Trying to unpickle estimator ColumnTransformer from version 1.7.0 when using version 1.7.1. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from '/tmp/s3models/lamarr-train/models/acceptance'.  Preprocessing: ðŸ‘Œ.Postprocessing: ðŸ˜ž.\n",
      "Check on the number of weights: âœ…! Original model: 166913. Collapsed model: 166913. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_54\n",
      "Received: inputs=('Tensor(shape=(1, 12))',)\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "acceptance = LamarrModel.from_saved_model_pb(\n",
    "    environ.get(\"ACCEPTANCE_MODEL\", \"/tmp/s3models/lamarr-train/models/acceptance/model.keras\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2c75ab40-3982-46b1-b319-f80d18d6a992",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from '/tmp/s3models/lamarr-train/models/efficiency'.  Preprocessing: ðŸ‘Œ.Postprocessing: ðŸ˜ž.\n",
      "Check on the number of weights: âœ…! Original model: 84740. Collapsed model: 84740. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_98\n",
      "Received: inputs=('Tensor(shape=(1, 12))',)\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "efficiency = LamarrModel.from_saved_model_pb(\n",
    "    environ.get(\"EFFICIENCY_MODEL\",  \"/tmp/s3models/lamarr-train/models/efficiency/model.keras\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e66fc93-9a1e-4782-9836-680ec4abcd10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from '/tmp/s3models/lamarr-train/models/resolution'.  Preprocessing: ðŸ‘Œ.Postprocessing: ðŸ‘Œ.\n",
      "Check on the number of weights: âœ…! Original model: 184329. Collapsed model: 184329. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_164\n",
      "Received: inputs=('Tensor(shape=(1, 140))',)\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "resolution = LamarrModel.from_saved_model_pb(\n",
    "        environ.get(\"RESOLUTION_MODEL\",  \"/tmp/s3models/lamarr-train/models/resolution/model.keras\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1432614b-8b7b-4e47-bf8a-77de5ccbca2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from '/tmp/s3models/lamarr-train/models/covariance'.  Preprocessing: ðŸ‘Œ.Postprocessing: ðŸ‘Œ.\n",
      "Check on the number of weights: âœ…! Original model: 152463. Collapsed model: 152463. \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/site-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "/usr/local/lib/python3.12/site-packages/keras/src/models/functional.py:237: UserWarning: The structure of `inputs` doesn't match the expected structure.\n",
      "Expected: keras_tensor_224\n",
      "Received: inputs=('Tensor(shape=(1, 143))',)\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "covariance = LamarrModel.from_saved_model_pb(\n",
    "        environ.get(\"COVARIANCE_MODEL\",  \"/tmp/s3models/lamarr-train/models/covariance/model.keras\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d895da2-ffdb-43d6-a398-8ac1250c9632",
   "metadata": {},
   "source": [
    "## Exporing the models to a shared object\n",
    "\n",
    "In order to export the models to a shared object we have to obtain\n",
    "a C code describing the models and then to compile the C code.\n",
    "For all operations supported by scikinC, we rely on code generation.\n",
    "For GANs, we will need to glue some steps together with custom C functions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a53d0d30-e28b-4bd3-b4a0-4829af911516",
   "metadata": {},
   "source": [
    "### Generation of C code\n",
    "\n",
    "For models like acceptance and efficiency where all the transform and the evaluations are pipelined in the same \"*direction*\" as they were trained, we can define *scikit-learn* Pipelines with the sequence of transformations and models and let scikinC to generate the glue between the preprocessing and DNN steps.\n",
    "\n",
    "For GANs this is not possible because we cannot pipeline the inverse of the postprocessing transformation `tY`.\n",
    "Besides, the input features have to be preprocessed, but the random noise should be injected directly in the generator model,\n",
    "breaking the Pipeline model.\n",
    "Hence, we generate C code for each step (preprocessing, model and postprocessing) separately.\n",
    "\n",
    "\n",
    "> Note. In addition, `scikinC` does not support pipelines of pipelines and the covariance GAN preprocessing step is defined as a pipeline, so even defining a inverse QuantileTransformer and injecting it in the pipeline would not be sufficient to automate the generation of the complete C code.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "af5cc553-765c-4b86-80c7-6c6af1a2a740",
   "metadata": {},
   "outputs": [],
   "source": [
    "models = dict(\n",
    "    # Acceptance model\n",
    "    acceptance=acceptance.pipeline,\n",
    "    \n",
    "    # Efficiency model\n",
    "    efficiency=efficiency.pipeline,\n",
    "    \n",
    "    # Resolution model\n",
    "    resolution_tX=resolution.tX,\n",
    "    resolution_dnn=resolution.collapsed_model,\n",
    "    resolution_tY=resolution.tY,\n",
    "    \n",
    "    # Covariance model\n",
    "    covariance_tX=covariance.tX,\n",
    "    covariance_dnn=covariance.collapsed_model,\n",
    "    covariance_tY=covariance.tY\n",
    ")\n",
    "\n",
    "# Retrieve the name of the file from the environment\n",
    "generated_c = environ.get(\"GENERATED_C_FILE\", \"exported/generated.C\")\n",
    "\n",
    "# Runs the scikinC code generator and save the output in the generated_c file\n",
    "print(scikinC.convert(models), file=open(generated_c, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81de3475-4c9e-4a3e-ac97-cdeca47b5d68",
   "metadata": {},
   "source": [
    "### Custom GAN pipelines\n",
    "\n",
    "The pipelines for the resolution and covariance models are defined in a separate file (gan_pipeline.c),\n",
    "reproduced below for completeness.\n",
    "\n",
    "> Note that scikinC operates with a separate call for each row or array, so all the arrays are 1D.\n",
    "\n",
    "In order to customize the pipeline with specific information from the model, such as \n",
    "the number of input and output features, or the dimensionality of the random noise,\n",
    "we use symbols that will be defined at compile time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e0a26833-70ae-4be8-8322-9b3f57b07e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "```c\n",
       "#define FLOAT_T float\n",
       "\n",
       "/**** Prototypes of functions automatically generated by scikinC ****/\n",
       "// Resolution steps\n",
       "FLOAT_T* resolution_tX         (FLOAT_T*, const FLOAT_T*);\n",
       "FLOAT_T* resolution_tY_inverse (FLOAT_T*, const FLOAT_T*);\n",
       "FLOAT_T* resolution_dnn        (FLOAT_T*, const FLOAT_T*);\n",
       "\n",
       "// Covariance steps\n",
       "FLOAT_T* covariance_tX         (FLOAT_T*, const FLOAT_T*);\n",
       "FLOAT_T* covariance_tY_inverse (FLOAT_T*, const FLOAT_T*);\n",
       "FLOAT_T* covariance_dnn        (FLOAT_T*, const FLOAT_T*);\n",
       "\n",
       "\n",
       "/**** Additional functions wrapping generated functions together ****/\n",
       "\n",
       "// Resolution entry point\n",
       "FLOAT_T* resolution (FLOAT_T* output, const FLOAT_T* input, const FLOAT_T* random)\n",
       "{\n",
       "    // Rename constants defined at compile time \n",
       "    const int nInputs = RESOLUTION_NUM_FEATURES;\n",
       "    const int nOutputs = RESOLUTION_NUM_OUTPUTS;\n",
       "    const int nRandom = COVARIANCE_NUM_RANDOM;\n",
       "\n",
       "    // Instanciate variables in the stack.\n",
       "    FLOAT_T buf[nInputs + nOutputs + nRandom];\n",
       "    int i;\n",
       "    \n",
       "    // Preprocessing\n",
       "    resolution_tX(buf, input);\n",
       "    \n",
       "    // Concatenate preprocessed features and random noise\n",
       "    for (i = 0; i < nRandom; ++i)\n",
       "        buf[i + nInputs] = random[i];\n",
       "    \n",
       "    // Execute the generator\n",
       "    resolution_dnn(buf, buf);\n",
       "    \n",
       "    // Apply the inverse postprocessing transformation\n",
       "    resolution_tY_inverse(output, buf);\n",
       "    \n",
       "    return output;\n",
       "}\n",
       "\n",
       "// Covariance entry point\n",
       "FLOAT_T* covariance (FLOAT_T* output, const FLOAT_T* input, const FLOAT_T* random)\n",
       "{\n",
       "    // Rename constants defined at compile time \n",
       "    const int nInputs = COVARIANCE_NUM_FEATURES;\n",
       "    const int nOutputs = COVARIANCE_NUM_OUTPUTS;\n",
       "    const int nRandom = COVARIANCE_NUM_RANDOM;\n",
       "    \n",
       "    // Instanciate variables in the stack.\n",
       "    FLOAT_T buf[nInputs + nOutputs + nRandom];\n",
       "    int i;\n",
       "    \n",
       "    // Preprocessing\n",
       "    covariance_tX(buf, input);\n",
       "    \n",
       "    // Concatenate preprcessed features and random noise\n",
       "    for (i = 0; i < nRandom; ++i)\n",
       "        buf[i + nInputs] = random[i];\n",
       "    \n",
       "    // Execute the DNN\n",
       "    covariance_dnn(buf, buf);\n",
       "    \n",
       "    // Apply the inverse postprocessing transformation\n",
       "    covariance_tY_inverse(output, buf);\n",
       "    \n",
       "    return output;\n",
       "}\n",
       "```"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "display(Markdown(f\"```c\\n{open('gan_pipelines.c').read()}\\n```\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "607272f0-6765-46a1-8f36-ed11a11a93b7",
   "metadata": {},
   "source": [
    "### Compilation to a shared object\n",
    "\n",
    "To compile the code we simply retrieve the missing information from the loaded models and we call gcc.\n",
    "\n",
    "The following compiler flags are worth a comment:\n",
    " * `-O3` defines a very high level of code optimization that may slighly reduce the floating point precision and \n",
    "   removes several checks on memory usage and allocation. For debugging, it should definitely be removed.\n",
    " * `-lm` links to the Standard C library with mathematic operations (as defined in the `math.h` header) and is needed for \n",
    "   the `tanh`, `log` and `exp` functions  used for activations\n",
    " * `--shared -fPIC` define the target binary as a shared library (instead of a static object or an executable application) to ease \n",
    "   importing the models in Gaudi at runtime.\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5a84ffcc-b860-474e-bcc9-58db5b3931a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "res_in = resolution.tX.n_features_in_\n",
    "res_out = resolution.tY.n_features_in_\n",
    "res_rnd = resolution.model.input_shape[1][1]\n",
    "cov_in = covariance.tX.n_features_in_\n",
    "cov_out = covariance.tY.n_features_in_\n",
    "cov_rnd = covariance.model.input_shape[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d5126f4-288b-4c74-bbf2-ae82755dbd0b",
   "metadata": {},
   "source": [
    "The complete compile command is reported below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c2b6a773-36c0-46b6-b920-c1d9fd17a195",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Compilation command:\n",
      " gcc  \\\n",
      "  exported/generated.C  \\\n",
      "  gan_pipelines.c  \\\n",
      "  -D RESOLUTION_NUM_FEATURES=12  \\\n",
      "  -D RESOLUTION_NUM_OUTPUTS=9  \\\n",
      "  -D RESOLUTION_NUM_RANDOM=128  \\\n",
      "  -D COVARIANCE_NUM_FEATURES=15  \\\n",
      "  -D COVARIANCE_NUM_OUTPUTS=15  \\\n",
      "  -D COVARIANCE_NUM_RANDOM=128  \\\n",
      "  -o ./exported/generated5739750.so  \\\n",
      "  -O3  \\\n",
      "  -lm  \\\n",
      "  --shared  \\\n",
      "  -fPIC\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "default_lib_path = generated_c.replace('.C', str(np.random.randint(0xFFFFFF)) + '.so')\n",
    "lib_path = environ.get(\"GENERATED_LIBRARY\", default_lib_path)\n",
    "\n",
    "import subprocess\n",
    "cmd = [\n",
    "    \"gcc\",\n",
    "    generated_c,\n",
    "    \"gan_pipelines.c\",\n",
    "    f\"-D RESOLUTION_NUM_FEATURES={res_in}\",\n",
    "    f\"-D RESOLUTION_NUM_OUTPUTS={res_out}\",\n",
    "    f\"-D RESOLUTION_NUM_RANDOM={cov_rnd}\",\n",
    "    f\"-D COVARIANCE_NUM_FEATURES={cov_in}\",\n",
    "    f\"-D COVARIANCE_NUM_OUTPUTS={cov_out}\",\n",
    "    f\"-D COVARIANCE_NUM_RANDOM={cov_rnd}\",\n",
    "    f\"-o ./{lib_path}\",\n",
    "    \"-O3\", \"-lm\", \"--shared\", \"-fPIC\"\n",
    "]\n",
    "print (\"Compilation command:\\n\", \"  \\\\\\n  \".join(cmd))\n",
    "os.system(\" \".join(cmd))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca8d2fe-8ea9-40b8-b5d3-f117458f4715",
   "metadata": {},
   "source": [
    "## Validation of the deployed models\n",
    "\n",
    "To ensure that the converted models are reasonably good we should validate them by comparing their output to the original models.\n",
    "Note in particular that they underwent two transformations:\n",
    " * they were collapsed from functional to sequential form\n",
    " * they were transpiled to C and compiled\n",
    " \n",
    "The scikinC package provides a helper class named `MLFunction` to wrap a function in a compiled C library and evaluate it on numpy arrays.\n",
    "A simple extension is need to also wrap our custom GAN pipelines which require passing random features as well. \n",
    "We will name such an extension `GanFunction`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "943d722b-4a87-490d-9387-4347172af6bf",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "MLFunction.__init__() missing 1 required positional argument: 'n_inputs'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 28\u001b[39m\n\u001b[32m     25\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m np.array(output_rows).astype(np.float64)\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Wraps to the functions for the various models using either the MLFunction or GanFunction wrappers\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m28\u001b[39m acceptance.deployed = \u001b[43mMLFunction\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlib_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43macceptance\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_outputs\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     29\u001b[39m efficiency.deployed = MLFunction(lib_path, \u001b[33m\"\u001b[39m\u001b[33mefficiency\u001b[39m\u001b[33m\"\u001b[39m, n_outputs=\u001b[32m4\u001b[39m)\n\u001b[32m     30\u001b[39m resolution.deployed = GanFunction(lib_path, \u001b[33m\"\u001b[39m\u001b[33mresolution\u001b[39m\u001b[33m\"\u001b[39m, n_outputs=res_out)\n",
      "\u001b[31mTypeError\u001b[39m: MLFunction.__init__() missing 1 required positional argument: 'n_inputs'"
     ]
    }
   ],
   "source": [
    "from scikinC.validation import MLFunction\n",
    "\n",
    "class GanFunction (MLFunction):\n",
    "    def __init__(self, lib_path, function_name, n_outputs, float_type=np.float32):\n",
    "        MLFunction.__init__(self, lib_path, function_name, n_outputs, float_type=np.float32)\n",
    "        # Define the C type of the arguments as C float (or numpy float32).\n",
    "        self._f.argtypes = [np.ctypeslib.ndpointer(dtype=np.float32) for _ in (1, 2, 3)]\n",
    "        \n",
    "    def __call__ (self, data_in, data_rnd):\n",
    "        # Define a buffer to store the output according to the size of the output array\n",
    "        obuf = np.empty(self.n_outputs, dtype=np.float32)\n",
    "        \n",
    "        # Convert the input features and the random noise to float32 \n",
    "        data_in_f = data_in.astype(np.float32)\n",
    "        data_rnd_f = data_rnd.astype(np.float32)\n",
    "        \n",
    "        # Run on each row of the input, and its random noise \n",
    "        # evaluate the C function and copies the output in a list\n",
    "        output_rows = []\n",
    "        for row_in, row_rnd in zip(data_in_f, data_rnd_f):\n",
    "            self._f(obuf, row_in, row_rnd)\n",
    "            output_rows.append(obuf.copy())\n",
    "            \n",
    "        # Build a 2D array from the list and set it back to float64 (numpy standard)\n",
    "        return np.array(output_rows).astype(np.float64)\n",
    "        \n",
    "# Wraps to the functions for the various models using either the MLFunction or GanFunction wrappers\n",
    "acceptance.deployed = MLFunction(lib_path, \"acceptance\", n_outputs=1)\n",
    "efficiency.deployed = MLFunction(lib_path, \"efficiency\", n_outputs=4)\n",
    "resolution.deployed = GanFunction(lib_path, \"resolution\", n_outputs=res_out)\n",
    "covariance.deployed = GanFunction(lib_path, \"covariance\", n_outputs=cov_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68298d23-91e6-4578-a1e6-f6683c01106e",
   "metadata": {},
   "source": [
    "To validate the full export mechanism we are going to evaluate the model using real training data and comparing:\n",
    " * the output of the original model as evaluated using the keras APIs\n",
    " * the output of the collapsed model as evaluated using the keras APIs\n",
    " * the output of the collapsed model, converted to C, compiled and wrapped with the `MLFunction` or `GanFunction` wrappers.\n",
    " \n",
    "Note that while for evaluating the models with the keras APIs we will rely on the preprocessed data stored on disk in the Preprocessing notebooks, to evaluate the whole pipeline deployed in C we will need to apply the inverse preprocessing transformation.\n",
    "\n",
    "For each output feature, we are drawing histograms:\n",
    " * for the overall distribution as obtained with the three models,\n",
    " * the absolute and relative difference between the result obtained with the collapsed and original models\n",
    " * the absolute and relative difference between the result obtained with the exported and original models\n",
    " \n",
    "We expect the comparison between the original and collapsed model to provide perfectly consistent results, unless of bugs introduced in the collapse algorithm. So this comparison is rather a sanity check than a real validation.\n",
    "Instead, discrepancies between the keras and C implementations is expected because of the floating point algebra and error propagation.\n",
    "Assess the entity of these discrepancies is not trivial and validating the result requires considering both the distributions of the absolute and relative errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3d3a382-2b74-4ccd-b497-19dedadd2129",
   "metadata": {},
   "outputs": [],
   "source": [
    "from validation_utils import invert_column_transformer\n",
    "from feather_io import FeatherReader\n",
    "\n",
    "\n",
    "def make_comparison_plot(title, y_py, y_py_collapsed, y_c):\n",
    "    y_py = y_py.flatten()\n",
    "    y_pyc = y_py_collapsed.flatten()\n",
    "    y_c = y_c.flatten()\n",
    "    \n",
    "    plt.figure(figsize=(15,3))\n",
    "    plt.subplot(1,3,1)\n",
    "    _, bins, _ = plt.hist(y_py, bins=50, label=\"Original\")\n",
    "    plt.hist(y_pyc, bins=bins, label=\"Collapsed\", histtype='step', linewidth=3)\n",
    "    plt.hist(y_c, bins=bins, label=\"Deployed\", histtype='step', linewidth=1)\n",
    "    plt.xlabel(\"Model response\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.yscale('log')\n",
    "\n",
    "    plt.subplot(1,3,2)\n",
    "    _, bins = np.histogram(100*(y_c - y_py), bins=50)\n",
    "    _, bins, _ = plt.hist(100.*(y_pyc - y_py), bins=bins, histtype='step', label=\"Collapse\", linewidth=2, zorder=2, color=\"#c00\")\n",
    "    _, bins, _ = plt.hist(100.*(y_c - y_py), bins=bins, label=\"Conversion\", zorder=1, color=\"#08c\")\n",
    "\n",
    "    plt.xlabel(\"Absolute conversion error\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend(title=\"Export step\")\n",
    "\n",
    "    \n",
    "    plt.subplot(1,3,3)\n",
    "    _, bins = np.histogram(100*(y_c - y_py)/y_py, bins=50)\n",
    "    _, bins, _ = plt.hist(100.*(y_pyc - y_py)/y_py, bins=bins, histtype='step', label=\"Collapse\", linewidth=2, zorder=2, color=\"#c00\")\n",
    "    _, bins, _ = plt.hist(100.*(y_c - y_py)/y_py, bins=bins, label=\"Conversion\", zorder=1, color=\"#08c\")\n",
    "\n",
    "    plt.xlabel(\"Relative conversion error [%]\")\n",
    "    plt.yscale('log')\n",
    "    plt.legend(title=\"Export step\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12bf5f68-bb3e-489b-9b98-7475868fc4af",
   "metadata": {},
   "source": [
    "### Validation of the acceptance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c040eeb-8009-4827-bc58-99de6f9181c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = FeatherReader(environ.get(\"ACCEPTANCE_TEST_DATA\", \"acceptance-validation\"))\n",
    "test_dataset = data_reader.as_dask_dataframe().head(10_000, npartitions=-1)\n",
    "pX = test_dataset[data_reader.features].values\n",
    "X = invert_column_transformer(acceptance.tX, pX)\n",
    "\n",
    "make_comparison_plot(\n",
    "    \"Acceptance\",\n",
    "    acceptance.model.predict(pX, verbose=False),\n",
    "    acceptance.collapsed_model.predict(pX, verbose=False),\n",
    "    acceptance.deployed(X)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f9ed8-8476-4a5e-9cdf-7e87adfde165",
   "metadata": {},
   "source": [
    "### Validation of the efficiency model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9826e14-f688-4df1-91bd-a6f62eecbbef",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = FeatherReader(environ.get(\"EFFICIENCY_TEST_DATA\", \"efficiency-validation\"))\n",
    "test_dataset = data_reader.as_dask_dataframe().head(10_000, npartitions=-1)\n",
    "pX = test_dataset[data_reader.features].values\n",
    "X = invert_column_transformer(efficiency.tX, pX)\n",
    "\n",
    "make_comparison_plot(\n",
    "    \"Efficiency as long tracks\",\n",
    "    efficiency.model.predict(pX, verbose=False)[:,1],\n",
    "    efficiency.collapsed_model.predict(pX, verbose=False)[:,1],\n",
    "    efficiency.deployed(X)[:,1]\n",
    ")\n",
    "\n",
    "make_comparison_plot(\n",
    "    \"Efficiency as upstream tracks\",\n",
    "    efficiency.model.predict(pX, verbose=False)[:,2],\n",
    "    efficiency.collapsed_model.predict(pX, verbose=False)[:,2],\n",
    "    efficiency.deployed(X)[:,2]\n",
    ")\n",
    "\n",
    "make_comparison_plot(\n",
    "    \"Efficiency as downstream tracks\",\n",
    "    efficiency.model.predict(pX, verbose=False)[:,3],\n",
    "    efficiency.collapsed_model.predict(pX, verbose=False)[:,3],\n",
    "    efficiency.deployed(X)[:,3]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70974ec6-4c9b-4a33-a813-647a48abf433",
   "metadata": {},
   "source": [
    "### Validation of the resolution model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d688c6-28e1-4df2-aa95-e1f73c05b2d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = FeatherReader(environ.get(\"RESOLUTION_TEST_DATA\", \"resolution-validation\"))\n",
    "test_dataset = data_reader.as_dask_dataframe().head(100_000, npartitions=-1)\n",
    "pX = test_dataset[data_reader.features].values\n",
    "X = invert_column_transformer(resolution.tX, pX)\n",
    "r = np.random.normal(0, 1, (len(X), 128))\n",
    "\n",
    "pY_py = resolution.model.predict((pX, r), batch_size=1000, verbose=False)\n",
    "Y_py = resolution.tY.inverse_transform(pY_py)\n",
    "pY_pyc = resolution.collapsed_model.predict(np.concatenate((pX, r), axis=1), batch_size=1000, verbose=False)\n",
    "Y_pyc = resolution.tY.inverse_transform(pY_pyc)\n",
    "\n",
    "Y_c = resolution.deployed(X, r)\n",
    "\n",
    "for iVar, var_name in enumerate(data_reader.labels):\n",
    "    make_comparison_plot(\n",
    "        var_name,\n",
    "        Y_py[:,iVar],\n",
    "        Y_pyc[:,iVar],\n",
    "        Y_c[:,iVar]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c447f733-3163-47c7-9214-8844af9875f5",
   "metadata": {},
   "source": [
    "### Validation of the covariance model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee884651-e470-4ed4-be99-a92f9b3b64ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_reader = FeatherReader(environ.get(\"COVARIANCE_TEST_DATA\", \"covariance-validation\"))\n",
    "test_dataset = data_reader.as_dask_dataframe().head(100_000, npartitions=-1)\n",
    "pX = test_dataset[data_reader.features].values\n",
    "X = invert_column_transformer(covariance.tX, pX)\n",
    "r = np.random.normal(0, 1, (len(X), 128))\n",
    "\n",
    "pY_py = covariance.model.predict((pX, r), batch_size=1000, verbose=False)\n",
    "Y_py = covariance.tY.inverse_transform(pY_py)\n",
    "pY_pyc = covariance.collapsed_model.predict(np.concatenate((pX, r), axis=1), batch_size=1000, verbose=False)\n",
    "Y_pyc = covariance.tY.inverse_transform(pY_pyc)\n",
    "\n",
    "Y_c = covariance.deployed(X, r)\n",
    "\n",
    "for iVar, var_name in enumerate(data_reader.labels):\n",
    "    make_comparison_plot(\n",
    "        var_name,\n",
    "        Y_py[:,iVar],\n",
    "        Y_pyc[:,iVar],\n",
    "        Y_c[:,iVar]\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6d8c5f-c924-4a35-b03e-8c05cee98751",
   "metadata": {},
   "source": [
    "# Conclusion\n",
    "\n",
    "In this notebook we discussed the procedure to export the trained models to a shared C library that can be easily \n",
    "imported at run time in Gaudi-based applications.\n",
    "\n",
    "The procedure used involves three steps:\n",
    " * collapsing the model from a functional form to a sequential model;\n",
    " * generating C code for the steps and completing it with the logic describing the sequence of the steps;\n",
    " * compiling the C code into a shared library.\n",
    " \n",
    "Finally, we have validated the procedure by comparing the results obtained running the original, collapsed and exported models on a subset of the training dataset.\n",
    "Studying the distributions we observe that the conversion introduces a small level of discrepancy between the exported and original data, probably because of floating point algebra or small discrepancies in the implementations of the quantile transformation.\n",
    "\n",
    "Still, the distributions are very well reproduced and the pathological entries are at the permil level or less, with relative errors of few percents that should not compromise the overall quality of the simulation.\n",
    "\n",
    "The most worrying case is probably the resolution where in $\\mathcal O(10^{-5})$ cases the events are pushed very far from the original value and might end up in unphysical regions. This might require to crop the resolution distribution to avoid outlayers.\n",
    "\n",
    "Tuning the hyperparameters of the quantile transformers is known to have a significan effect on these discrepancies. However, increasing the number of bins also slows down the evaluation, so that a trade-off between speed and \"safety\" must be found."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe2759d-7e53-4f73-ac25-e9f83413c18c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lb-trksim-train-2",
   "language": "python",
   "name": "lb-trksim-train-2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
